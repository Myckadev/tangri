# Stack Open-Meteo Lambda: Kafka + Spark + HDFS + Airflow (Postgres + init) + FastAPI + Streamlit + Prometheus + Grafana
x-env: &env
  env_file:
    - .env
  environment:
    TZ: ${TZ:-Europe/Paris}

networks:
  lambda-net:
    driver: bridge

volumes:
  hdfs-nn: {}
  hdfs-dn: {}
  prometheus-data: {}
  grafana-data: {}
  # Airflow/Postgres
  postgres-data: {}
  airflow-logs: {}
  airflow-dags: {}
  airflow-plugins: {}

services:
  zookeeper:
    image: bitnami/zookeeper:3.9
    <<: *env
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
    networks: [lambda-net]

  kafka:
    image: bitnami/kafka:3.7
    <<: *env
    depends_on: [zookeeper]
    healthcheck:
      test: [ "CMD-SHELL", "kafka-topics.sh --bootstrap-server kafka:9092 --list >/dev/null 2>&1" ]
      interval: 10s
      timeout: 5s
      retries: 12
    environment:
      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092
      - KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE=false
      - ALLOW_PLAINTEXT_LISTENER=yes
    ports:
      - "9092:9092"
    networks: [lambda-net]

  kafka-ui:
    image: provectuslabs/kafka-ui:v0.7.2
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181
    ports:
      - "8081:8080"
    depends_on:
      - kafka
      - zookeeper
    networks: [ lambda-net ]

  # Exporter Prometheus pour Kafka (lag consumer, etc.)
  kafka-exporter:
    image: danielqsj/kafka-exporter:latest
    <<: *env
    command: ["--kafka.server=kafka:9092", "--zookeeper.server=zookeeper:2181"]
    depends_on:
      kafka:
        condition: service_healthy
    ports:
      - "9308:9308"
    networks: [lambda-net]

  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    <<: *env
    environment:
      - CLUSTER_NAME=lambda
    ports:
      - "9870:9870"   # HDFS NameNode UI (sert aussi pour WebHDFS)
    volumes:
      - hdfs-nn:/hadoop/dfs/name
    networks: [lambda-net]

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    <<: *env
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
    depends_on: [namenode]
    ports:
      - "9864:9864"
    volumes:
      - hdfs-dn:/hadoop/dfs/data
    networks: [lambda-net]

  spark-master:
    image: bitnami/spark:3.5
    <<: *env
    user: "0:0"
    environment:
      - SPARK_MODE=master
      - HOME=/opt/spark
      - HADOOP_USER_NAME=root
      - USER=root
    depends_on: [datanode]
    ports:
      - "8080:8080"   # Spark Master UI
      - "7077:7077"   # Spark Master RPC
    volumes:
      - ./spark:/opt/spark_jobs
    networks: [lambda-net]

  spark-worker-1:
    image: bitnami/spark:3.5
    <<: *env
    user: "0:0"
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - HOME=/opt/spark
      - HADOOP_USER_NAME=root
      - USER=root
    depends_on: [spark-master]
    volumes:
      - ./spark:/opt/spark_jobs
    networks: [lambda-net]

  # ---------- Airflow + Postgres ----------
  postgres:
    image: postgres:15-alpine
    <<: *env
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-data:/var/lib/postgresql/data
    networks: [lambda-net]

  airflow-init:
    build:
      context: ./airflow
      args:
        AIRFLOW_VERSION: "3.0.6"
        PYTHON_VERSION: "3.11"
    env_file: [.env]
    user: "${AIRFLOW_UID:-50000}:0"
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__AUTH_MANAGER: airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/dags
      AIRFLOW__CORE__STORE_SERIALIZED_DAGS: "True"
      AIRFLOW__CORE__MIN_SERIALIZED_DAG_UPDATE_INTERVAL: "5"
      AIRFLOW__API__AUTH_BACKENDS: airflow.api.auth.backend.basic_auth   # << enable API auth
    depends_on:
      - postgres
    volumes:
      - airflow-dags:/opt/airflow/dags
      - airflow-logs:/opt/airflow/logs
      - airflow-plugins:/opt/airflow/plugins
      - ./airflow/dags:/opt/airflow/dags
      - ./spark:/opt/spark_jobs
      - ./conf:/opt/conf
    entrypoint: >
      bash -lc "
        airflow db migrate &&
        airflow users create --username admin --password admin --firstname a --lastname b --role Admin --email a@b.c || true
      "
    networks: [lambda-net]

  airflow-webserver:
    build:
      context: ./airflow
      args:
        AIRFLOW_VERSION: "3.0.6"
        PYTHON_VERSION: "3.11"
    env_file: [.env]
    user: "${AIRFLOW_UID:-50000}:0"
    environment:
      # --- Core / DB ---
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/dags
      AIRFLOW__CORE__STORE_SERIALIZED_DAGS: "True"
      AIRFLOW__CORE__MIN_SERIALIZED_DAG_UPDATE_INTERVAL: "5"
      AIRFLOW__CORE__DAGBAG_IMPORT_TIMEOUT: "120"
      # --- Logging (clé pour voir les logs dans l’UI) ---
      AIRFLOW__LOGGING__BASE_LOG_FOLDER: /opt/airflow/logs
      AIRFLOW__LOGGING__REMOTE_LOGGING: "False"
      AIRFLOW__LOGGING__TASK_LOG_READER: task
      AIRFLOW__LOGGING__LOGGING_LEVEL: DEBUG
      AIRFLOW__LOGGING__FAB_LOGGING_LEVEL: DEBUG
      # --- UI / Divers ---
      AIRFLOW__WEBSERVER__EXPOSE_STACKTRACE: "True"
      # --- Tes env Spark ---
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_QUERY_SCRIPT: /opt/spark_jobs/run_query.py
    depends_on:
      postgres:
        condition: service_started
      airflow-init:
        condition: service_completed_successfully
    command: ["bash","-lc","exec airflow api-server"]
    volumes:
      - airflow-dags:/opt/airflow/dags
      - airflow-logs:/opt/airflow/logs
      - airflow-plugins:/opt/airflow/plugins
      - ./airflow/dags:/opt/airflow/dags
      - ./spark:/opt/spark_jobs
      - ./conf:/opt/conf
    ports:
      - "8088:8080"
    healthcheck:
      test: [ "CMD-SHELL","curl -fsS http://localhost:8080/api/v2/monitor/health | grep -q '\"status\":\"healthy\"'" ]
      interval: 10s
      timeout: 5s
      retries: 30
    networks: [lambda-net]

  airflow-scheduler:
    build:
      context: ./airflow
      args:
        AIRFLOW_VERSION: "3.0.6"
        PYTHON_VERSION: "3.11"
    env_file: [.env]
    user: "${AIRFLOW_UID:-50000}:0"
    environment:
      # --- Core / DB ---
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__AUTH_MANAGER: airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/dags
      AIRFLOW__CORE__STORE_SERIALIZED_DAGS: "True"
      AIRFLOW__CORE__MIN_SERIALIZED_DAG_UPDATE_INTERVAL: "5"
      AIRFLOW__CORE__DAGBAG_IMPORT_TIMEOUT: "120"
      # --- Logging (identique au webserver) ---
      AIRFLOW__LOGGING__BASE_LOG_FOLDER: /opt/airflow/logs
      AIRFLOW__LOGGING__REMOTE_LOGGING: "False"
      AIRFLOW__LOGGING__TASK_LOG_READER: task
      AIRFLOW__LOGGING__LOGGING_LEVEL: DEBUG
      AIRFLOW__LOGGING__FAB_LOGGING_LEVEL: DEBUG
      # --- API auth si tu pilotes via l’API ---
      AIRFLOW__API__AUTH_BACKENDS: airflow.api.auth.backend.basic_auth
      # --- Tes env Spark ---
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_QUERY_SCRIPT: /opt/spark_jobs/run_query.py
      # (optionnel mais pratique)
      AIRFLOW__WEBSERVER__EXPOSE_STACKTRACE: "True"
    depends_on:
      airflow-webserver:
        condition: service_healthy
    command: ["bash","-lc","exec airflow scheduler"]
    volumes:
      - airflow-dags:/opt/airflow/dags
      - airflow-logs:/opt/airflow/logs
      - airflow-plugins:/opt/airflow/plugins
      - ./airflow/dags:/opt/airflow/dags
      - ./spark:/opt/spark_jobs
      - ./conf:/opt/conf
    networks: [lambda-net]

  airflow-dag-processor:
    build:
      context: ./airflow
      args:
        AIRFLOW_VERSION: "3.0.6"
        PYTHON_VERSION: "3.11"
    env_file: [ .env ]
    user: "${AIRFLOW_UID:-50000}:0"
    environment:
      # --- Core / DB ---
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__AUTH_MANAGER: airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/dags
      AIRFLOW__CORE__STORE_SERIALIZED_DAGS: "True"
      AIRFLOW__CORE__MIN_SERIALIZED_DAG_UPDATE_INTERVAL: "5"
      AIRFLOW__CORE__DAGBAG_IMPORT_TIMEOUT: "120"
      # --- Logging (identique) ---
      AIRFLOW__LOGGING__BASE_LOG_FOLDER: /opt/airflow/logs
      AIRFLOW__LOGGING__REMOTE_LOGGING: "False"
      AIRFLOW__LOGGING__TASK_LOG_READER: task
      AIRFLOW__LOGGING__LOGGING_LEVEL: DEBUG
      AIRFLOW__LOGGING__FAB_LOGGING_LEVEL: DEBUG
      # --- API auth si besoin ---
      AIRFLOW__API__AUTH_BACKENDS: airflow.api.auth.backend.basic_auth
      # --- Tes env Spark ---
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_QUERY_SCRIPT: /opt/spark_jobs/run_query.py
      AIRFLOW__WEBSERVER__EXPOSE_STACKTRACE: "True"
    depends_on:
      airflow-webserver:
        condition: service_healthy
    command: [ "bash","-lc","exec airflow dag-processor" ]
    volumes:
      - airflow-dags:/opt/airflow/dags
      - airflow-logs:/opt/airflow/logs
      - airflow-plugins:/opt/airflow/plugins
      - ./airflow/dags:/opt/airflow/dags
      - ./spark:/opt/spark_jobs
      - ./conf:/opt/conf
    networks: [ lambda-net ]


  # ---------------------------------------------------------------

  backend:
    build: ./backend
    image: openmeteo-backend:latest
    <<: *env
    environment:
      - KAFKA_BROKER=${KAFKA_BROKER:-kafka:9092}
      - TOPIC_Q_REQ=${TOPIC_Q_REQ:-weather.queries.requests}
      - TOPIC_Q_RES=${TOPIC_Q_RES:-weather.queries.results}
      - TOPIC_HOURLY=${TOPIC_HOURLY:-weather.hourly.flattened}     # << pour métriques temps réel Grafana
    volumes:
      - ./conf:/app/conf
      - ./backend:/app
    depends_on: [kafka]
    ports:
      - "8000:8000"
    networks: [lambda-net]

  producer:
    build: ./producer
    image: openmeteo-producer:latest
    <<: *env
    environment:
      - KAFKA_BROKER=${KAFKA_BROKER:-kafka:9092}
      - TOPIC_RAW=${TOPIC_RAW:-weather.raw.openmeteo}
      - CITIES_FILE=/app/conf/cities.json
    volumes:
      - ./conf:/app/conf
    depends_on: [kafka, backend]
    restart: unless-stopped
    networks: [lambda-net]

  streamlit:
    build: ./streamlit
    image: openmeteo-streamlit:latest
    <<: *env
    depends_on: [backend]
    environment:
      - BACKEND_URL=http://backend:8000
      - PRODUCER_URL=http://producer:8000
      - WEBHDFS_URL=http://namenode:9870/webhdfs/v1
    ports:
      - "8501:8501"
    networks: [lambda-net]

  query-runner:
    build: ./query_runner
    image: openmeteo-query-runner:latest
    <<: *env
    environment:
      - KAFKA_BROKER=${KAFKA_BROKER:-kafka:9092}
      - TOPIC_Q_REQ=${TOPIC_Q_REQ:-weather.queries.requests}
      - TOPIC_Q_RES=${TOPIC_Q_RES:-weather.queries.results}
      - AIRFLOW_API_URL=http://airflow-webserver:8080/api/v2
      - AIRFLOW_API_USERNAME=admin
      - AIRFLOW_API_PASSWORD=admin
      - AIRFLOW_DAG_ID=query_runner
      - AIRFLOW_BASE_URL=http://airflow-webserver:8080     # ou 8088 côté host, mais ici on parle réseau docker
      - AIRFLOW_TOKEN_URL=http://airflow-webserver:8080/auth/token
    depends_on:
      kafka:
        condition: service_healthy
      airflow-webserver:
        condition: service_healthy
    restart: unless-stopped
    networks: [lambda-net]

  prometheus:
    image: prom/prometheus:v2.53.0
    <<: *env
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    ports:
      - "9090:9090"
    networks: [lambda-net]

  cadvisor:
    image: gcr.io/cadvisor/cadvisor:v0.49.1
    <<: *env
    privileged: true
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
    ports:
      - "8085:8080"
    networks: [lambda-net]

  grafana:
    image: grafana/grafana:10.4.2
    <<: *env
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana-data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning
    ports:
      - "3000:3000"
    depends_on: [prometheus]
    networks: [lambda-net]
